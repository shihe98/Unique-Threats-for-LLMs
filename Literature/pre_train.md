# Pretrain LLMs
## Privcacy Risks
* Private Data in Corpora ![Static Badge](https://img.shields.io/badge/Unique-red)
1. *[ProPILE: Probing Privacy Leakage in Large Language Models](https://arxiv.org/abs/2307.01881)* ![Static Badge](https://img.shields.io/badge/NIPS'23-blue)
2. *[Quantifying Memorization Across Neural Language Models](https://arxiv.org/abs/2202.07646)* ![Static Badge](https://img.shields.io/badge/ICLR'23-blue)

* Data Extraction Attack ![Static Badge](https://img.shields.io/badge/Common-red)
1. *[Scalable Extraction of Training Data from (Production) Language Models](https://arxiv.org/abs/2311.17035)*
2. *[ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation](https://aclanthology.org/2023.acl-long.709/)* ![Static Badge](https://img.shields.io/badge/ACL'23-blue)
3. *[Canary Extraction in Natural Language Understanding Models](https://arxiv.org/abs/2203.13920)* ![Static Badge](https://img.shields.io/badge/ACL'22-blue)

## Security Risks
* Toxic Data in Corpora ![Static Badge](https://img.shields.io/badge/Unique-red)
1. *[Toxicity in ChatGPT: Analyzing Persona-assigned Language Models](https://arxiv.org/abs/2304.05335)* ![Static Badge](https://img.shields.io/badge/EMNLP'23-blue)
2. *[TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models](https://arxiv.org/abs/2306.11507)*
3. *[Probing Toxic Content in Large Pre-Trained Language Models](https://aclanthology.org/2021.acl-long.329/)* ![Static Badge](https://img.shields.io/badge/ACL'21-blue)
4. *[On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning](https://arxiv.org/abs/2212.08061)* ![Static Badge](https://img.shields.io/badge/ACL'23-blue)

* Poison Attack in Pre-training ![Static Badge](https://img.shields.io/badge/Common-red)<br>
**A. Data Poisoning**
1. *[ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger](https://aclanthology.org/2024.naacl-long.165/)* ![Static Badge](https://img.shields.io/badge/NAACL'24-blue)
2. *[Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models](https://aclanthology.org/2023.emnlp-main.757/)* ![Static Badge](https://img.shields.io/badge/EMNLP'23-blue)
3. *[Concealed Data Poisoning Attacks on NLP Models](https://aclanthology.org/2021.naacl-main.13/)* ![Static Badge](https://img.shields.io/badge/NAACL'21-blue)<br>
**B. Model Poisoning**
## Countermeasure
1. [A Comprehensive Study on Machine Learning](https://example.com/paper.pdf)
2. [A Comprehensive Study on Machine Learning](https://example.com/paper.pdf)
3. [A Comprehensive Study on Machine Learning](https://example.com/paper.pdf)
4. [A Comprehensive Study on Machine Learning](https://example.com/paper.pdf)
5. [A Comprehensive Study on Machine Learning](https://example.com/paper.pdf)
6. [A Comprehensive Study on Machine Learning](https://example.com/paper.pdf)

