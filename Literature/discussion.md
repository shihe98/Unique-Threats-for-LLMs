# Further Work and Discussion
## Federated Learning for LLMs
* *A. Federated Learning Framework*
1. *[FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models](https://arxiv.org/abs/2310.10049)*
2. *[Low-Parameter Federated Learning with Large Language Models](https://arxiv.org/abs/2307.13896)*
3. *[Training Large-Vocabulary Neural Language Models by Private Federated Learning for Resource-Constrained Devices](https://arxiv.org/abs/2207.08988)* ![Static Badge](https://img.shields.io/badge/ICASSP'23-red)
4. *[FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models](https://aclanthology.org/2023.findings-acl.632/)* ![Static Badge](https://img.shields.io/badge/ACL'23-red)
* *B. Threats for Federated Learning*
1. *[Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning](https://arxiv.org/abs/2312.05720)* ![Static Badge](https://img.shields.io/badge/NIPS_Workshop'23-red)
2. *[LAMP: Extracting Text from Gradients with Language Model Priors](https://arxiv.org/abs/2202.08827)* ![Static Badge](https://img.shields.io/badge/NIPS'22-red)
3. *[Exploiting Unintended Feature Leakage in Collaborative Learning](https://ieeexplore.ieee.org/document/8835269)* ![Static Badge](https://img.shields.io/badge/S&P(Oakland)'19-red)
## Watermarks for LLMs
* *A. Model Copyright*
1. *[Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning](https://arxiv.org/abs/2402.14883)*
2. *[Protecting Language Generation Models via Invisible Watermarking](https://arxiv.org/abs/2302.03162)* ![Static Badge](https://img.shields.io/badge/ICML'23-red)
3. *[Watermarking Classification Dataset for Copyright Protection](https://arxiv.org/abs/2305.13257)*
4. *[Protecting Intellectual Property of Language Generation APIs with Lexical Watermark](https://arxiv.org/abs/2112.02701)* ![Static Badge](https://img.shields.io/badge/AAAI'22-red)
* *B. Output Copyright*
1. *[REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models](https://arxiv.org/abs/2310.12362)* ![Static Badge](https://img.shields.io/badge/Usenix_Security'24-red)
2. *[An Unforgeable Publicly Verifiable Watermark for Large Language Models](https://arxiv.org/abs/2310.08920)* ![Static Badge](https://img.shields.io/badge/ICLR'24-red)
3. *[Embarrassingly Simple Text Watermarks](https://arxiv.org/abs/2310.08920)*
4. *[Robust Multi-bit Natural Language Watermarking through Invariant Features](https://arxiv.org/abs/2305.01904)* ![Static Badge](https://img.shields.io/badge/ACL'23-red)
5. *[A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)* ![Static Badge](https://img.shields.io/badge/ICML'23-red)
6. *[Robust Distortion-free Watermarks for Language Models](https://arxiv.org/abs/2307.15593)* ![Static Badge](https://img.shields.io/badge/TMLR'23-red)
## Machine Unlearning for LLMs
1. *[Who's Harry Potter? Approximate Unlearning in LLMs](https://arxiv.org/abs/2310.02238)*
2. *[Machine unlearning for generative AI](https://www.ingentaconnect.com/contentone/hsp/airwa/2024/00000003/00000001/art00005)*
3. *[Large Language Model Unlearning](https://arxiv.org/abs/2310.10683)* ![Static Badge](https://img.shields.io/badge/Socially_Responsible_Language_Modelling_Research'23-red)
