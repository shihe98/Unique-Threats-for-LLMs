# Unique-Threats-for-LLMs

# The Life Cycle of LLM

## Pre-training LLMs
Model developers collect a large corpus as a pre-training dataset, including books, web pages, conversational texts, and code. They then use large-scale, Transformer-based networks and advanced training algorithms, enabling the models to learn rich language knowledge from vast amounts of unlabeled texts. After obtaining the pre-trained LLM, developers upload it to open-source community platforms to gain profits. We consider three malicious entities: contributors, developers, and users.
<p align="center">
    <img src="./img/pre_train.png" alt="pre_train" width="900" height="240">
</p>

* [Privacy Risks](./img/pre_train.png)
* [Security Risks](./img/pre_train.png)
* [Countermeasure](./img/pre_train.png)

## Fine-tuning LLMs
Users customize LLMs for specific NLP tasks. They download pre-trained LLMs from open-source platforms and fine-tune them on customized datasets. There are three fine-tuning methods: supervised learning, instruction-tuning, and alignment-tuning. The first method is the commonly used training algorithm. For the second method, the instruction is in natural language format, containing a task description, an optional demonstration, and an input-output pair. Through a sequence-to-sequence loss, instruction-tuning helps LLMs understand and generalize to unseen tasks. The third method aligns LLMs' outputs with human preferences, such as usefulness, honesty, and harmlessness. Reinforcement learning from human feedback (RLHF) can meet these goals. We consider two malicious entities: contributors and third parties.
<p align="center">
    <img src="./img/pre_train.png" alt="pre_train" width="900" height="240">
</p>

* [Privacy Risks](./img/pre_train.png)
* [Security Risks](./img/pre_train.png)
* [Countermeasure](./img/pre_train.png)

## RAG System

* [Privacy Risks](./img/pre_train.png)
* [Security Risks](./img/pre_train.png)
* [Countermeasure](./img/pre_train.png)

## Deploying LLMs

* [Privacy Risks](./img/pre_train.png)
* [Security Risks](./img/pre_train.png)
* [Countermeasure](./img/pre_train.png)

## Deploying LLM-based Agents

* [Privacy Risks](./img/pre_train.png)
* [Security Risks](./img/pre_train.png)
* [Countermeasure](./img/pre_train.png)
